{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from timm.models import create_model\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, Normalize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(img):\n",
    "    transform_fn = Compose([Resize(249, 3), CenterCrop(224), ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    return transform_fn(img)\n",
    "\n",
    "def show_img(img):\n",
    "    img = np.asarray(img)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_img2(img1, img2, alpha=0.8):\n",
    "    img1 = np.asarray(img1)\n",
    "    img2 = np.asarray(img2)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img1)\n",
    "    plt.imshow(img2, alpha=alpha)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def my_forward_wrapper(attn_obj):\n",
    "    def my_forward(x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = attn_obj.qkv(x).reshape(B, N, 3, attn_obj.num_heads, C // attn_obj.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * attn_obj.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = attn_obj.attn_drop(attn)\n",
    "        attn_obj.attn_map = attn\n",
    "        attn_obj.cls_attn_map = attn[:, :, 0, 2:]\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = attn_obj.proj(x)\n",
    "        x = attn_obj.proj_drop(x)\n",
    "        return x\n",
    "    return my_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('')\n",
    "x = to_tensor(img)\n",
    "\n",
    "model = create_model('deit_small_distilled_patch16_224', pretrained=True)\n",
    "model.blocks[-1].attn.forward = my_forward_wrapper(model.blocks[-1].attn)\n",
    "\n",
    "y = model(x.unsqueeze(0))\n",
    "attn_map = model.blocks[-1].attn.attn_map.mean(dim=1).squeeze(0).detach()\n",
    "cls_weight = model.blocks[-1].attn.cls_attn_map.mean(dim=1).view(14, 14).detach()\n",
    "\n",
    "img_resized = x.permute(1, 2, 0) * 0.5 + 0.5\n",
    "cls_resized = F.interpolate(cls_weight.view(1, 1, 14, 14), (224, 224), mode='bilinear').view(224, 224, 1)\n",
    "\n",
    "show_img(img)\n",
    "show_img(attn_map)\n",
    "show_img(cls_weight)\n",
    "show_img(img_resized)\n",
    "show_img2(img_resized, cls_resized, alpha=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('motionformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45cc188df1ef5582c38d97e85df64391f6a7344a93618f2b66e6d75e78f0aaaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
